# CS6540_HCI
Human Computer Interaction course at the University of Utah.
This repo contains the activities completed during the course.
# 1. [Recommendation_System_Trust_&_Safety](https://github.com/vishvadesai9/CS6540_HCI/blob/main/Recommendation_System_Trust_and_Safety.pdf)
Research Question: How do personalized recommender systems affect human behavior with respect to trust and safety according to user perception across different platforms?

Methods used: Online Surveys (Amazon MTurks) & Interviews

Team: [Vishva Desai](https://www.linkedin.com/in/vishva-desai/), [Khawar Murad Ahmed](khawar.ahmed@utah.edu) & [Thomas Greger](thomas.greger@utah.edu)

This project is an attempt to find out more about how users percieve recommendation systems with regards to trust & safety.

# 2. [Final_Project_PPT](https://github.com/vishvadesai9/CS6540_HCI/blob/main/Final_Project_PPT.pdf)
PPT presented in class for the research paper.

# 3. [Interviews](https://github.com/vishvadesai9/CS6540_HCI/blob/main/Interview_Report.pdf)
Interviews conducted for the research project on 6 participants. 

[Individual Report](https://github.com/vishvadesai9/CS6540_HCI/blob/main/HCI_Interview_WriteUp.pdf) 

[Interview Recordings](https://drive.google.com/drive/folders/1NMaV3sJaT7j694zCDkHfl0KNy4VMZyI1?usp=share_link)


# 4. [Online Surveys](https://github.com/vishvadesai9/CS6540_HCI/blob/main/Online_Questionnares_Report.pdf)
Online Survey report for the research project. 

[Survey](https://utah.sjc1.qualtrics.com/jfe/form/SV_0qBWD8RrM1XaXmS) compiled using Qualtrics and deployed using Amazon MTurks.

# 5. [Think Aloud](https://github.com/vishvadesai9/CS6540_HCI/blob/main/HCI_Think_Aloud_Report.pdf)
Think aloud protocols consist of observing a user working with an interface while encouraging them to "think-aloud"; to say what they are thinking and wondering at each moment. Another way to put it would be; making Observations as the user vocalizes their actions.

Reference: Boren and Ramsey's [Thinking Aloud: reconciling theory and practice](https://ieeexplore.ieee.org/document/867942/)

# 6. [Heuristic Evaluation](https://github.com/vishvadesai9/CS6540_HCI/blob/main/HCI_Heuristic_Evaluation%20(1).pdf)
Heuristic evaluation is a method where people use a set of heuristics, frequently design principles, to identify strengths and potential problems of a user interface or interaction.
Heuristic evaluations can be performed by members of the design team or by outside experts. Typically multiple (3-5) people perform the evaluation of the interface individually and then synthesize their results into a single report.

Reference: [Jakob Nielsen's design principles.](https://www.nngroup.com/articles/ten-usability-heuristics/)
